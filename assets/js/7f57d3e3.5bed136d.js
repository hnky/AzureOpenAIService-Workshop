"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[593],{4137:(e,t,o)=>{o.d(t,{Zo:()=>d,kt:()=>h});var n=o(7294);function r(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function a(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,n)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?a(Object(o),!0).forEach((function(t){r(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function l(e,t){if(null==e)return{};var o,n,r=function(e,t){if(null==e)return{};var o,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)o=a[n],t.indexOf(o)>=0||(r[o]=e[o]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)o=a[n],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(r[o]=e[o])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},d=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var o=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(o),m=r,h=u["".concat(s,".").concat(m)]||u[m]||c[m]||a;return o?n.createElement(h,i(i({ref:t},d),{},{components:o})):n.createElement(h,i({ref:t},d))}));function h(e,t){var o=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=o.length,i=new Array(a);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<a;p++)i[p]=o[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,o)}m.displayName="MDXCreateElement"},6839:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var n=o(7462),r=(o(7294),o(4137));const a={},i="Explore AI Models",l={unversionedId:"Explore-AI-Models",id:"Explore-AI-Models",title:"Explore AI Models",description:"Before you begin this section, navigate to setup part of the workshop.",source:"@site/docs/10-Explore-AI-Models.md",sourceDirName:".",slug:"/Explore-AI-Models",permalink:"/AzureOpenAIService-Workshop/Explore-AI-Models",draft:!1,editUrl:"https://github.com/revodavid/OpenAI-Lab-UCB/tree/main/docs/10-Explore-AI-Models.md",tags:[],version:"current",sidebarPosition:10,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Setup",permalink:"/AzureOpenAIService-Workshop/Setup"},next:{title:"Understanding LLM's",permalink:"/AzureOpenAIService-Workshop/Understanding-LLMs"}},s={},p=[{value:"Deployed models available:",id:"deployed-models-available",level:2},{value:"Which model should I use?",id:"which-model-should-i-use",level:2}],d={toc:p},u="wrapper";function c(e){let{components:t,...o}=e;return(0,r.kt)(u,(0,n.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"explore-ai-models"},"Explore AI Models"),(0,r.kt)("p",null,"Before you begin this section, navigate to ",(0,r.kt)("a",{parentName:"p",href:"Setup"},"setup")," part of the workshop. "),(0,r.kt)("h2",{id:"deployed-models-available"},"Deployed models available:"),(0,r.kt)("p",null,"In this workshop you will be using the ",(0,r.kt)("inlineCode",{parentName:"p"},"gpt-35-turbo")," model, an instance of the OpenAI ChatGPT model."),(0,r.kt)("p",null,"In this workshop, we will occasionally mention GPT-4, the latest model from OpenAI, but we will not use it."),(0,r.kt)("p",null,"You can find details about these models and other models available in Azure OpenAI Service at ",(0,r.kt)("a",{parentName:"p",href:"https://aka.ms/oai/models"},"https://aka.ms/oai/models"),". "),(0,r.kt)("p",null,"There you will learn that:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"gpt-35-turbo is currently available 10 regions, has a Max Request limit of 4,096 tokens (16k tokens for gpt-35-turbo-16k), and is based on training data up to September 2021."),(0,r.kt)("li",{parentName:"ul"},"gpt-4 is available in 9 regions, has a Max Request limit of 8,192 tokens (or 32,768 tokens for the gpt-4-32k variant), and is based on training data up to September 2021.")),(0,r.kt)("h2",{id:"which-model-should-i-use"},"Which model should I use?"),(0,r.kt)("p",null,"There are many considerations when choosing a model, including cost, availability, performance, and capability. But as a general guide, we recommend the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'Start with gpt-35-turbo. This model is very economical, has good performance, and despite the "ChatGPT" name can be used for a wide range of tasks beyond chat and conversation.'),(0,r.kt)("li",{parentName:"ul"},"If you need to generate more than 4,096 tokens, or need to support larger prompts, you will need to use gpt-35-turbo-16k, gpt-4 or gpt-4-32k. These models are more expensive and can be slower, and have limited availability, but they are the most powerful models available today. ",(0,r.kt)("em",{parentName:"li"},"Find out more about tokens later on in this workshop.")),(0,r.kt)("li",{parentName:"ul"},"For tasks like search, clustering, recommendations and anomaly detection consider an embeddings model. An embedding is vector of numbers that can be easily utilized by computers. The embedding is an information dense representation of the semantic meaning of a piece of text. The distance between two embeddings in the vector space is correlated with semantic similarity, for example, if two texts are similar, then their vector representations should also be similar.  search, clustering, recommendations and anomaly detection."),(0,r.kt)("li",{parentName:"ul"},"DALL-E is a model that generates images from text prompts that the user provides. Unlike the models about the output is different, image not text like chat."),(0,r.kt)("li",{parentName:"ul"},"Finally, Whisper can be used for speech to text capabilities, often used to transcribe audio files. The model is trained on a large dataset of English audio and text. The model is optimized for transcribing audio files that contain speech in English. The model can also be used to transcribe audio files that contain speech in other languages. The output of the model is English text. Best for quickly transcribing audio files one at a time, translate audio from other languages into English or providing a prompt to the model to guide the output.")))}c.isMDXComponent=!0}}]);